<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Alexander Williams </title> <meta name="author" content="Alexander Williams"> <meta name="description" content="I am a PhD Student at Queen Mary University of London in the UKRI Centre for Doctoral Training in Artificial Intelligence and Music researching user-driven deep music generation in digital audio workstations, in collaboration with Sony CSL and under the supervision of Mathieu Barthet and Stefan Lattner. I received an MSc. by Research in Artificial Intelligence and Robotics from Swansea University in 2022 for research into deep reinforcement learning for robotic aerospace manufacturing, and a First Class BSc. in Computer Science from the University of Liverpool in 2018. From 2018 to 2023, I was a Research Assistant in the ASTUTE (Advanced Sustainable Manufacturing Technologies) operation at Swansea University, where I undertook collaborative research, development, and innovation projects with manufacturing companies across Wales in the remit of Industry 4.0. concepts including AI, robotics, and manufacturing systems engineering. My research interests include artificial intelligence, computational creativity for electronic dance music, music information retrieval, robotics and industrial automation, human-computer interaction, industry 4.0, and the Internet of Things. "> <meta name="keywords" content="Artificial-Intelligence, Music-Information-Retrieval, EDM, DJ, Researcher"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%99%82%E2%80%8D%E2%86%95%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alexjameswilliams.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alexander</span> Williams </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown active"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus <span class="sr-only">(current)</span> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item active" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="williams_towards_2025" class="col-sm-8"> <div class="title">Towards Music Industry 5.0: Perspectives on Artificial Intelligence</div> <div class="author"> <em>Alexander Williams</em>, Stefan Lattner, and Mathieu Barthet </div> <div class="periodical"> <em>In Artificial Intelligence for Music Workshop at the 39th Annual AAAI Conference on Artificial Intelligence</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Artificial Intelligence (AI) is a disruptive technology that is transforming many industries including the music industry. Recently, the concept of Industry 5.0. has been proposed emphasising principles of sustainability, resilience, and human-centricity to address current shortcomings in Industry 4.0. and its associated technologies, including AI. In line with these principles, this paper puts forward a position for ethical AI practices in the music industry. We outline the current state of AI in the music industry and its wider ethical and legal issues through an analysis and discussion of contemporary case studies. We list current commercial applications of AI in music, collect a range of perspectives on AI in the industry from diverse stakeholders, and comment on existing and forthcoming regulatory frameworks and industry initiatives. Resultingly, we provide several timely research directions, practical recommendations, and commercial opportunities to aid the transition to a human-centric, resilient, and sustainable music industry 5.0. This work particularly focuses on western music industry case studies in the European Union (EU), United States of America (US), and United Kingdom (UK), but many of the issues raised are universal. While this work is not exhaustive, we nevertheless hope it guides researchers, businesses, and policy makers to develop responsible frameworks for deploying and regulating AI in the music industry.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="williams_deep_2024" class="col-sm-8"> <div class="title">Deep Learning-based Audio Representations for the Analysis and Visualisation of Electronic Dance Music DJ Mixes</div> <div class="author"> <em>Alexander Williams</em>, Haokun Tian, Stefan Lattner, Mathieu Barthet, and Charalampos Saitis </div> <div class="periodical"> <em>In AES International Symposium on AI and the Musician</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Electronic dance music (EDM), produced using computers and electronic instruments, is a collection of musical subgenres that emphasise timbre and rhythm over melody and harmony. It is usually presented through the medium of DJing, where tracks are curated and mixed sequentially to offer unique listening and dancing experiences. However, unlike key and tempo annotations, DJs still rely on audition rather than metadata to examine and select tracks with complementary audio content. In this work, we investigate the use of deep learning-based representations (Complex Autoencoder and OpenL3) for analysing and visualising audio content on a corpus of DJ mixes with approximate transition timestamps and compare them with signal processing-based representations (joint time-frequency scattering transform and mel-frequency cepstral coefficients). Representations are computed once per second and visualised with UMAP dimensionality reduction. We propose heuristics based on the identification of observed patterns in visualisations and time-sensitive Euclidean distances in the representation space to compute DJ transition lengths, transition smoothness, and inter-song, song-to-song, and full-mix audio content consistency using audio representations along with rough DJ transition timestamps. Our method enables the visualisation of variations within music tracks, facilitating the analysis of DJ mixes and individual EDM tracks. This approach supports musicians in making informed creative decisions based on such visualisations. We share our code, dataset annotations, computed audio representations, and trained CAE model. We encourage researchers and music enthusiasts alike to analyse their own music using our tools: https://github.com/ alexjameswilliams/EDMAudioRepresentations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="williams_using_2024" class="col-sm-8"> <div class="title">Invited Talk: Using AI to Augment Creativity in Electronic Dance Music</div> <div class="author"> <em>Alexander Williams</em> </div> <div class="periodical"> Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>AI is a disruptive technology that is being applied to numerous areas of the music industry. However, the reception from creators is mixed. Some fear that AI may be used to automate opportunities away from human creators, while others see AI as a tool that can be used in beneficial ways. This talk will showcase recent research applications of AI to augment human creativity in two electronic dance music case studies: 1) co-creative music artwork generation; and 2) analysing DJ mixes to inform human creative practice. It will also touch on some of the ethical and legal concerns raised by these AI technologies in music.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="williams_invited_2024-1" class="col-sm-8"> <div class="title">Invited Talk: Applications and Perspectives of AI in the Music Industry</div> <div class="author"> <em>Alexander Williams</em> </div> <div class="periodical"> Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This seminar will introduce a co-creative process for generating images to accompany a piece of music using pre-trained neural network models. According to this process, visuals are influenced not only by the audio of a piece of music, but also by a corpus of illustrations and prompts recommended by the user, in order to anchor the generated content in a creative approach. The seminar will also discuss important topic of ethics and responsible innovation regarding AI and music. We will address the issues and initiatives in industry and governance guiding researchers, companies, and policy makers in the development of fair and responsible frameworks for the deployment and regulation of AI in the music industry.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="williams_workshop_2024" class="col-sm-8"> <div class="title">Tutorial: Model Pipelines for AI Music Artwork Generation</div> <div class="author"> <em>Alexander Williams</em> </div> <div class="periodical"> Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this workshop, we will study how to combine state-of-the-art deep neural network models for music-to-text conversion (MusiCNN and LP-MusicCaps), image-to-text (CLIP Interrogator), key word detection via natural language processing (KeyBERT), and text-to-image (Stable Diffusion) to recommend, through generation, visuals for a piece of music. The Colab notebook used during the workshop will include a sequencer to produce musical sequences from randomly selected audio samples. Images illustrating these musical sequences will be generated using pre-trained models. This workshop will also cover Python libraries for recording energy use during computation (CodeCarbon)”</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="matallah_reinforcement_2023" class="col-sm-8"> <div class="title">A Reinforcement Learning Approach to Powertrain Optimisation</div> <div class="author"> Hocine Matallah, Asad Javied, <em>Alexander Williams</em>, Ashraf Fahmy Abdo, and Fawzi Belblidia </div> <div class="periodical"> <em>In Sustainable Design and Manufacturing</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-981-19-9205-6_24" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>A strategy to reduce computation time and improve minimisation performance in the context of optimisation of battery electric vehicle power trains is provided, motivated by constraints in the motor manufacturing business. This paper proposes a holistic design exploration approach to investigate and identify the optimal powertrain concept for cars based on the component costs and energy consumption costs. Optimal powertrain design and component sizes are determined by analysing various powertrain configuration topologies, as well as single and multi-speed gearbox combinations. The impact of powertrain combinations on vehicle attributes and total costs is investigated further. Multi-objective optimisation in this domain considers a total of 29 component parameters comprised of differing modalities. We apply a novel reinforcement learning-based framework to the problem of simultaneous optimisation of these 29 parameters and demonstrate the feasibility of this optimisation method for this domain. Our results show that, in comparison to single rear motor setups, multi-motor systems offer better vehicle attributes and cheaper total costs. We also show that load points with front and back axle motors may be shifted to a greater efficiency zone to achieve decreased energy consumption and expenses.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Alexander_J._Williams_142340499" class="col-sm-8"> <div class="title">Sound-and-Image-Informed Music Artwork Generation Using Text-to-Image Models</div> <div class="author"> Alexander J. Williams, Stefan Lattner, and Mathieu Barthet </div> <div class="periodical"> <em>In Music Recommender Systems Workshop at the 17th ACM Conference on Recommender Systems</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Music and its accompanying artwork have a symbiotic relationship. While some artists are involved in both domains, the creation of music and artwork require different skill sets. The development of deep generative models for music and image generation has potential to democratise these mediums and make multi-modal creation more accessible for casual creators and other stakeholders. In this work, we propose a co-creative pipeline for the generation of images to accompany a musical piece. This pipeline utilises state-of-the-art models for music-to-text, image-to-text, and subsequently text-to-image generation to recommend, via generation, visuals for a piece of music that are informed not only by the audio of a musical piece, but also a user-recommended corpus of artworks and prompts to give a meaningful grounding in the generated material. We demonstrate the potential of our pipeline using a corpus of material from artists with strongly connected visual and musical identities, and make it available in the form of a Python notebook for users to easily generate their own musical and visual compositions using their chosen corpus - available here: https://github.com/alexjameswilliams/Music-Text-To-Image-Generation</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="williams_survey_2021" class="col-sm-8"> <div class="title">Survey of Energy Harvesting Technologies for Wireless Sensor Networks</div> <div class="author"> Alexander J. Williams, Matheus F. Torquato, Ian M. Cameron, Ashraf A. Fahmy, and Johann Sienz </div> <div class="periodical"> <em>IEEE Access</em>, Sep 2021 </div> <div class="periodical"> Conference Name: IEEE Access </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ACCESS.2021.3083697" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Energy harvesting (EH) technologies could lead to self-sustaining wireless sensor networks (WSNs) which are set to be a key technology in Industry 4.0. There are numerous methods for small-scale EH but these methods differ greatly in their environmental applicability, energy conversion characteristics, and physical form which makes choosing a suitable EH method for a particular WSN application challenging due to the specific application-dependency. Furthermore, the choice of EH technology is intrinsically linked to non-trivial decisions on energy storage technologies and combinatorial architectures for a given WSN application. In this paper we survey the current state of EH technology for small-scale WSNs in terms of EH methods, energy storage technologies, and EH system architectures for combining methods and storage including multi-source and multi-storage architectures, as well as highlighting a number of other optimisation considerations. This work is intended to provide an introduction to EH technologies in terms of their general working principle, application potential, and other implementation considerations with the aim of accelerating the development of sustainable WSN applications in industry.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="torquato_cascade_2021" class="col-sm-8"> <div class="title">Cascade Optimisation of Battery Electric Vehicle Powertrains</div> <div class="author"> Matheus F. Torquato, Kayalvizhi Lakshmanan, Natalia Narożańska, Ryan Potter, <em>Alexander Williams</em>, Fawzi Belblidia, Ashraf A. Fahmy, and Johann Sienz </div> <div class="periodical"> <em>In Procedia Computer Science</em>, Jan 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.procs.2021.08.061" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Motivated by challenges in the motor manufacturing industry, a solution to reduce computation time and improve minimisation performance in the context of optimisation of battery electric vehicle powertrain is presented. We propose a cascade optimisation method that takes advantage of two different vehicle models: the proprietary YASA MATLAB® vehicle model and a Python machine learning-based vehicle model derived from the proprietary model. Gearbox type, powertrain configuration and motor parameters are included as input variables to the objective function explored in this work while constraints related to acceleration time and top speed must be met. The combination of these two models in a constrained optimisation genetic algorithm managed to both reduce the amount of computation time required and achieve more optimal target values relating to minimising vehicle total cost than either the proprietary or machine learning model alone. The coarse-to-fine approach utilised in the cascade optimisation was proven to be mainly responsible for the improved optimisation result. By using the final population of the machine learning vehicle model optimisation as the initial population of the following simulation-based minimisation, the initial time-consuming search to produce a population satisfying all domain constraints was practically eliminated. The obtained results showed that the cascade optimisation was able to reduce the computation time by 53% and still achieve a minimisation value 14% lower when compared to the YASA Vehicle Model Optimisation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="williams_real-time_2021" class="col-sm-8"> <div class="title">Real-Time Hybrid Visual Servoing of a Redundant Manipulator via Deep Reinforcement Learning</div> <div class="author"> <em>Alexander Williams</em> </div> <div class="periodical"> <em>Swansea University</em>, Feb 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Fixtureless assembly may be necessary in some manufacturing tasks and environments due to various constraints but poses challenges for automation due to non-deterministic characteristics not favoured by traditional approaches to industrial au-tomation. Visual servoing methods of robotic control could be effective for sensitive manipulation tasks where the desired end-effector pose can be ascertained via visual cues. Visual data is complex and computationally expensive to process but deep reinforcement learning has shown promise for robotic control in vision-based manipu-lation tasks. However, these methods are rarely used in industry due to the resources and expertise required to develop application-specific systems and prohibitive training costs. Training reinforcement learning models in simulated environments offers a number of benefits for the development of robust robotic control algorithms by reducing training time and costs, and providing repeatable benchmarks for which algorithms can be tested, developed and eventually deployed on real robotic control environments. In this work, we present a new simulated reinforcement learning envi-ronment for developing accurate robotic manipulation control systems in fixtureless environments. Our environment incorporates a contemporary collaborative industrial robot, the KUKA LBR iiwa, with the goal of positioning its end effector in a generic fixtureless environment based on a visual cue. Observational inputs are comprised of the robotic joint positions and velocities, as well as two cameras, whose positioning reflect hybrid visual servoing with one camera attached to the robotic end-effector, and another observing the workspace respectively. We propose a state-of-the-art deep reinforcement learning approach to solving the task environment and make prelimi-nary assessments of the efficacy of this approach to hybrid visual servoing methods for the defined problem environment. We also conduct a series of experiments exploring the hyperparameter space in the proposed reinforcement learning method. Although we could not prove the efficacy of a deep reinforcement approach to solving the task environment with our initial results, we remain confident that such an approach could be feasible to solving this industrial manufacturing challenge and that our contributions in this work in terms of the novel software provide a good basis for the exploration of reinforcement learning approaches to hybrid visual servoing in accurate manufacturing contexts.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Alexander Williams. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>